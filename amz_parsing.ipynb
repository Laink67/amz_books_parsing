{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from lxml import html\n",
    "import requests\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36'\n",
    "headers = {'User-Agent': user_agent}\n",
    "\n",
    "ERROR = 'ERROR'\n",
    "site = 'https://www.amazon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id(domen, item, max_tries=3):\n",
    "    url = f'{site}.{domen}/s?k={item}&s=review-rank'\n",
    "    ok_response_code = '200'\n",
    "    \n",
    "    def _get_response(rs, cnt_=0):\n",
    "        cnt = cnt_\n",
    "        while ok_response_code not in str(rs):\n",
    "            cnt += 1\n",
    "            if cnt > max_tries:\n",
    "                return rs\n",
    "            try:\n",
    "                rs = requests.get(url, headers)\n",
    "            except Exception as e:\n",
    "                print(f\"Can't connect to {item} by exception {e}\")\n",
    "                continue\n",
    "            return _get_response(rs, cnt)\n",
    "        return rs\n",
    "    try:\n",
    "        page = requests.get(url, headers = headers)\n",
    "    except Exception as e:\n",
    "        print(f\"Can't connect to {item} by exception {e}\")\n",
    "        return ERROR\n",
    "        \n",
    "    page = _get_response(page, cnt_=0)\n",
    "    \n",
    "    if ok_response_code not in str(page):\n",
    "        return ERROR\n",
    "    \n",
    "    parser = html.fromstring(page.content)\n",
    "    \n",
    "    xpath_div  = './/div[@class=\"s-result-item s-asin sg-col-0-of-12 sg-col-16-of-20 sg-col s-widget-spacing-small sg-col-12-of-16\"]/@data-asin'\n",
    "    ids = parser.xpath(xpath_div)\n",
    "    \n",
    "    if len(ids) == 0:\n",
    "        xpath_div  = './/div[@class=\"sg-col-4-of-12 s-result-item s-asin sg-col-4-of-16 sg-col s-widget-spacing-small sg-col-4-of-20\"]/@data-asin'\n",
    "        ids = parser.xpath(xpath_div)\n",
    "    \n",
    "    if len(ids) == 0:\n",
    "        xpath_div  = './/div[@class=\"sg-col-4-of-24 sg-col-4-of-12 s-result-item s-asin sg-col-4-of-16 sg-col s-widget-spacing-small sg-col-4-of-20\"]/@data-asin'\n",
    "        ids = parser.xpath(xpath_div)\n",
    "    \n",
    "    try:\n",
    "        return ids[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Can't find by exception: e\")\n",
    "        \n",
    "    return ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10351"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_init_df = pd.read_csv('books_2.csv', usecols=['title', 'authors'], error_bad_lines=False)\n",
    "\n",
    "books_init_df['title_']  = books_init_df['title'].copy()\n",
    "# Убираем записи в скобках, чтобы поиск был без ошибки\n",
    "books_init_df.loc[(books_init_df['title'].str.contains('Potter')) | (books_init_df['title'].str.contains('Hitchhiker')), 'title_'] = books_init_df\\\n",
    ".loc[(books_init_df['title'].str.contains('Potter')) | (books_init_df['title'].str.contains('Hitchhiker')), 'title'].str.replace(r' \\(.*$', '')\n",
    "books_init_df['title_author'] = books_init_df['title_'].copy()\n",
    "# Добавляем к книгам автора, чтобы поиск был более точным\n",
    "books_init_df.loc[(~books_init_df['title_'].str.contains('Potter')) & (~books_init_df['title_'].str.contains('Hitchhiker')), 'title_author'] = books_init_df['title_'] + ' ' + books_init_df['authors']\n",
    "#  Убираем 'Harry Potter and', чтобы поиск был без ошибки\n",
    "books_init_df['title_author'] = books_init_df['title_'].replace('Harry Potter and ','', regex=True)\n",
    "# Добавляем слово 'book', чтобы искал только книги, а не фильмы\n",
    "books = ('book ' + books_init_df['title_author']).unique()\n",
    "\n",
    "# books = books_init_df['title_'].unique()\n",
    "\n",
    "len(books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_items(items, domen):\n",
    "    items_dict = {}\n",
    "    for item in tqdm(items):\n",
    "        cur_id = get_id(domen, item=item)\n",
    "        if cur_id != ERROR:\n",
    "            items_dict[item] = {}\n",
    "            items_dict[item]['id'] = cur_id\n",
    "    return items_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review(domen, item_id, n_pages=10):\n",
    "    res_dict = {}\n",
    "    for n_page in range(1, n_pages+1):\n",
    "        url = f'https://www.amazon.{domen}/product-reviews/{item_id}?pageNumber={n_page}&sortBy=recent'\n",
    "        try:\n",
    "            page = requests.get(url, headers = headers)\n",
    "        except Exception as e:\n",
    "            print(f\"Can't connect to by EXCEPTION: {e}\")\n",
    "            return ERROR\n",
    "        parser = html.fromstring(page.content)\n",
    "\n",
    "        xpath_reviews = '//div[@data-hook=\"review\"]'\n",
    "        reviews = parser.xpath(xpath_reviews)\n",
    "        \n",
    "        xpath_rating  = './/span[@class=\"a-icon-alt\"]//text()'\n",
    "    \n",
    "        xpath_title   = './/a[@data-hook=\"review-title\"]//span//text()'\n",
    "        xpath_style   = './/a[@data-hook=\"book-style\"]//span//text()'\n",
    "        xpath_author_id  = './/span[@class=\"a-profile-id\"]//text()'\n",
    "        xpath_author  = './/span[@class=\"a-profile-name\"]//text()'\n",
    "        xpath_date    = './/span[@data-hook=\"unixReviewTime\"]//text()'\n",
    "        xpath_body    = './/span[@data-hook=\"review-body\"]//text()'\n",
    "        xpath_verified = './/span[@data-hook=\"verified-statement\"]//text()'\n",
    "\n",
    "        page_dict = {}\n",
    "        i = 0\n",
    "        for review in reviews:\n",
    "            rating = review.xpath(xpath_rating)\n",
    "            title = review.xpath(xpath_title)\n",
    "            style = review.xpath(xpath_style)\n",
    "                \n",
    "            author = review.xpath(xpath_author)\n",
    "            author_id = review.xpath(xpath_author_id)\n",
    "            date = review.xpath(xpath_date)\n",
    "            body = review.xpath(xpath_body)\n",
    "            verified_ = review.xpath(xpath_verified)\n",
    "            verified = True if len(helpful_) != 0 else False\n",
    "            review_dict = {'asin' : item_id,\n",
    "                           'reviewerID' : author_id,\n",
    "                           'reviewerName': author,\n",
    "                           'overall': rating,\n",
    "                           'unixReviewTime': date,\n",
    "                           'summary': title,\n",
    "                           'style': style,\n",
    "                           'reviewText': body,\n",
    "                           'verified': verified}\n",
    "            \n",
    "            res_dict[len(res_dict) + i] = review_dict\n",
    "            i += 1\n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_items(books, 'com').to_csv('new_book_ids.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                         | 0/1056 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOMEN: co.jp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|███████████▍                                                                 | 157/1056 [37:48<2:03:12,  8.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't connect to book The Dark City (Eliot Ness  #1) by EXCEPTION: ('Connection aborted.', ConnectionResetError(10054, 'Удаленный хост принудительно разорвал существующее подключение', None, 10054, None))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|███████████████████▋                                                       | 277/1056 [1:06:22<2:48:22, 12.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't connect to book Diary of a Spider by EXCEPTION: ('Connection aborted.', ConnectionResetError(10054, 'Удаленный хост принудительно разорвал существующее подключение', None, 10054, None))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|██████████████████████████████████████████████████████████████████▍          | 911/1056 [3:25:21<30:56, 12.80s/it]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cur_items = pd.read_csv('new_book_ids.csv')\n",
    "cur_items = cur_items.rename({'Unnamed: 0': 'item'}, axis=1)\n",
    "res_df = pd.DataFrame(columns=['asin', 'reviewerID', 'reviewerName', 'style','overall', 'unixReviewTime', 'summary', 'reviewText', 'verified'])\n",
    "total_memory = 0\n",
    "\n",
    "for domen in ['com']:    \n",
    "    thr = cur_items.shape[0] / 8\n",
    "    for begin, end in [(1056 * 4, 1056 * 5), (1056 * 5, 1056 * 6), (1056 * 6, 1056 * 7), (1056 * 7, cur_items.shape[0])]:\n",
    "        res_df = pd.DataFrame(columns=['asin', 'reviewerID', 'reviewerName','style','overall', 'unixReviewTime', 'summary', 'reviewText', 'verified'])\n",
    "        for index, row in tqdm(cur_items.iloc[begin:end].iterrows(),  total=cur_items.iloc[begin:end].shape[0]):\n",
    "            item_id = row['asin']\n",
    "            book_dict = get_review(domen, item_id, n_pages=15)\n",
    "            if book_dict == ERROR:\n",
    "                continue\n",
    "\n",
    "            cur_df = pd.DataFrame.from_dict(book_dict, orient='index')\n",
    "\n",
    "            res_df = res_df.append(cur_df)\n",
    "            del cur_df\n",
    "        \n",
    "        res_df.to_json(f'new_{domen}_books_{begin}_{end}.json')\n",
    "\n",
    "        del res_df\n",
    "        gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
